const axios = require('axios');

const OLLAMA_HOST = process.env.OLLAMA_HOST || 'http://localhost:11434';
// üá´üá∑ Mixtral 8x7B - Meilleur mod√®le pour le fran√ßais (Mistral AI, France)
const MODEL_NAME = process.env.OLLAMA_MODEL || 'mixtral';

async function getOllamaResponse(prompt, history = [], isComplexReasoning = false) {
    try {
        console.log(`üîµ Ollama: Envoi requ√™te au mod√®le ${MODEL_NAME}...`);
        const startTime = Date.now();
        
        // üöÄ CONTEXTE MASSIF - Param√®tres optimis√©s pour g√©rer √©norm√©ment de contexte
        // Mixtral 8x7B peut g√©rer jusqu'√† 32K tokens de contexte (‚âà 24K mots)
        const numCtx = parseInt(process.env.OLLAMA_CONTEXT_SIZE) || 32768; // 32K tokens par d√©faut
        const numPredict = isComplexReasoning ? 512 : 256; // R√©ponses plus longues et d√©taill√©es
        const temperature = isComplexReasoning ? 0.6 : 0.7; // L√©g√®rement plus cr√©atif
        
        // Format messages for Ollama avec timeout adaptatif (Mixtral est plus lent mais meilleur)
        // ‚ö†Ô∏è Avec le contexte massif, Mixtral peut prendre plus de temps
        const timeout = isComplexReasoning ? 300000 : 180000; // 3 minutes simple, 5 minutes complexe
        
        const response = await axios.post(`${OLLAMA_HOST}/api/generate`, {
            model: MODEL_NAME,
            prompt: prompt,
            stream: false,
            options: {
                // üöÄ CONTEXTE MASSIF - Param√®tres pour g√©rer √©norm√©ment de m√©moire
                num_ctx: numCtx,              // 32K tokens de contexte (√âNORME)
                temperature: temperature,
                top_p: 0.9,
                top_k: 40,
                num_predict: numPredict,
                // üá´üá∑ Param√®tres optimis√©s pour fran√ßais
                repeat_penalty: 1.1,      // √âvite les r√©p√©titions (important en fran√ßais)
                presence_penalty: 0.5,    // Favorise la diversit√© du vocabulaire
                frequency_penalty: 0.3,   // R√©duit les r√©p√©titions de mots
                // üß† Param√®tres de m√©moire et performance
                num_thread: 8,            // Utilise 8 threads pour la vitesse
                num_gpu: 99,              // Utilise tous les GPUs disponibles
                stop: ['\n\n', 'User:', 'Utilisateur:', 'Assistant:', 'Bot:', 'Toi:']
            }
        }, {
            timeout: timeout
        });
        
        const duration = ((Date.now() - startTime) / 1000).toFixed(2);
        console.log(`‚úÖ Ollama: R√©ponse re√ßue en ${duration}s`);
        
        let text = (response.data && response.data.response) ? response.data.response : '';
        if (typeof text !== 'string') text = String(text || '');
        text = text.replace(/^(Bot:|Toi:|Assistant:)/i, '').trim();
        // Ne garder que la premi√®re ligne utile
        if (text.includes('\n')) {
            text = text.split(/\r?\n/).map(l => l.trim()).filter(l => l.length > 0)[0] || text.trim();
        }
        if (!text) {
            text = "Je n'ai pas compris, reformule.";
        }
        return text;
    } catch (error) {
        console.error('‚ùå Erreur Ollama:', error.message);
        if (error.code === 'ECONNABORTED') {
            return "D√©sol√©, l'IA met trop de temps √† r√©pondre... üò¥";
        }
        return "D√©sol√©, je n'arrive pas √† joindre l'IA...";
    }
}

module.exports = { getOllamaResponse };
